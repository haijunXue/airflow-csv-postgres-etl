# ğŸ“Š Cloud-Based ETL Pipeline with Airflow, PostgreSQL & dbt (AWS)

## ğŸš€ Project Overview
This project implements a **production-style end-to-end ETL pipeline deployed on an AWS EC2 instance** using **Apache Airflow**, **PostgreSQL**, and **dbt**.

Sales data is extracted from CSV files, transformed using Python (pandas), loaded **incrementally** into a PostgreSQL data warehouse, validated with data quality checks, and transformed into analytics-ready tables using dbt.

The entire workflow is orchestrated and monitored with Apache Airflow.

---

## â˜ï¸ Cloud Deployment (AWS)

The pipeline runs on a **Linux-based AWS EC2 instance**, simulating a real-world production data platform.

**Infrastructure**
- AWS EC2 (Linux)
- Apache Airflow
- PostgreSQL (Data Warehouse)
- dbt (Analytics Engineering)

**Responsibilities**
- EC2 environment setup
- Python virtual environment management
- Airflow DAG orchestration and scheduling
- PostgreSQL connection and schema management
- End-to-end pipeline monitoring

---

## ğŸ— Architecture

CSV Files
â†“
Apache Airflow (ETL Orchestration)
â†“
PostgreSQL (Raw Data Layer)
â†“
dbt (Staging & Fact Models)

---

## ğŸ”§ Tech Stack

- Apache Airflow
- Python (pandas)
- PostgreSQL
- dbt
- SQL
- AWS EC2
- Linux & Bash

---

## âœ¨ Key Features

- Incremental data loading using metadata tracking
- Idempotent inserts (`ON CONFLICT DO NOTHING`)
- Data quality checks (empty tables, NULL primary keys)
- dbt staging and fact models
- Fully automated DAG execution

---

## ğŸ“ Project Structure

â”œâ”€â”€ dags/
â”‚ â””â”€â”€ csv_to_postgres_etl.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sales.csv
â”œâ”€â”€ dbt/
â”‚ â””â”€â”€ airflow_dbt/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ models/
â”‚ â”œâ”€â”€ staging/
â”‚ â”‚ â””â”€â”€ stg_sales.sql
â”‚ â””â”€â”€ marts/
â”‚ â””â”€â”€ fct_sales.sql
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸ”„ ETL Workflow

### 1ï¸âƒ£ Extract & Transform
- Reads sales data from CSV
- Converts date fields
- Calculates `amount_with_tax`

### 2ï¸âƒ£ Incremental Load
- Loads only new records based on `last_loaded_date`
- Stores ETL metadata in a dedicated table

### 3ï¸âƒ£ Data Quality Checks
- Verifies that data exists
- Ensures primary key integrity

### 4ï¸âƒ£ dbt Transformations
- Builds staging models
- Creates analytics-ready fact tables

---

## â–¶ï¸ How to Run

### 1. Install dependencies
```bash
pip install apache-airflow pandas psycopg2-binary dbt-postgres
2. Start Airflow
airflow db init
airflow scheduler
airflow webserver
3. Trigger the DAG
airflow dags trigger csv_to_postgres_etl
ğŸ“Œ Future Improvements

Bulk inserts for large datasets

dbt tests and snapshots

Dockerized deployment

CI/CD for dbt models

Cloud-native services (S3, RDS)

Skills:
Airflow Â· AWS Â· dbt Â· SQL Â· Python Â· Data Engineering
